name: Deploy Databricks Bundle

on:
  workflow_dispatch:
    inputs:
      target:
        description: "Environment to deploy (dev | qa | prod)"
        required: true
        default: "dev"
        type: choice
        options: [dev, qa, prod]
      run_job:
        description: "Run the job after deploy?"
        required: true
        default: "true"
        type: choice
        options: ["true", "false"]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      # 1) Checkout the repo
      - name: Checkout
        uses: actions/checkout@v4

      # 2) Setup Python with pip cache using requirements.txt
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"
          cache-dependency-path: "requirements.txt"

      # 3) Install Python deps (SDK only; bundles use the new CLI below)
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true

      # 4) Install NEW Databricks CLI (bundle support) dynamically via GitHub Releases API
      - name: Install Databricks CLI (new)
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y jq unzip tar

          # Get latest linux_amd64 asset URL (zip or tar.gz)
          ASSET_URL="$(curl -s https://api.github.com/repos/databricks/cli/releases/latest \
            | jq -r '.assets[] | select(.name | test("linux_amd64\\.(zip|tar\\.gz)$")) | .browser_download_url' | head -n1)"

          if [ -z "$ASSET_URL" ] || [ "$ASSET_URL" = "null" ]; then
            echo "❌ Could not determine Databricks CLI asset URL from GitHub Releases."
            exit 1
          fi

          echo "Downloading Databricks CLI from: $ASSET_URL"
          mkdir -p "$HOME/.databricks/bin"
          curl -L -o /tmp/databricks_cli_asset "$ASSET_URL"

          # Extract (supports .zip or .tar.gz)
          FILETYPE="$(file -b --mime-type /tmp/databricks_cli_asset)"
          if [[ "$ASSET_URL" == *.zip ]]; then
            unzip -o /tmp/databricks_cli_asset -d "$HOME/.databricks/bin"
          elif [[ "$ASSET_URL" == *.tar.gz ]]; then
            tar -xzf /tmp/databricks_cli_asset -C "$HOME/.databricks/bin"
          else
            # fallback by mime
            if [[ "$FILETYPE" == "application/zip" ]]; then
              unzip -o /tmp/databricks_cli_asset -d "$HOME/.databricks/bin"
            else
              tar -xzf /tmp/databricks_cli_asset -C "$HOME/.databricks/bin"
            fi
          fi

          chmod +x "$HOME/.databricks/bin/databricks"
          echo "$HOME/.databricks/bin" >> $GITHUB_PATH
          databricks version

      # 5) Map secrets to env based on target and export globally (YOUR secret names kept)
      - name: Set environment variables
        run: |
          TARGET="${{ github.event.inputs.target }}"
          echo "TARGET=$TARGET" >> $GITHUB_ENV

          if [ "$TARGET" = "dev" ]; then
            echo "DATABRICKS_HOST=${{ secrets.DATABRICKS_HOST_DEV }}"   >> $GITHUB_ENV
            echo "DATABRICKS_TOKEN=${{ secrets.DATABRICKS_TOKEN_DEV }}" >> $GITHUB_ENV
          elif [ "$TARGET" = "qa" ]; then
            echo "DATABRICKS_HOST=${{ secrets.DATABRICKS_HOST_QA }}"    >> $GITHUB_ENV
            echo "DATABRICKS_TOKEN=${{ secrets.DATABRICKS_TOKEN_QA }}"  >> $GITHUB_ENV
          elif [ "$TARGET" = "prod" ]; then
            echo "DATABRICKS_HOST=${{ secrets.DATABRICKS_HOST_PROD }}"  >> $GITHUB_ENV
            echo "DATABRICKS_TOKEN=${{ secrets.DATABRICKS_TOKEN_PROD }}">> $GITHUB_ENV
          else
            echo "❌ Unknown target: $TARGET"; exit 1
          fi

          # quick sanity check
          if [ -z "$DATABRICKS_HOST" ] || [ -z "$DATABRICKS_TOKEN" ]; then
            echo "❌ Missing DATABRICKS_* secrets for target '$TARGET'"; exit 1
          fi

      # 6) Validate bundle
      - name: Validate bundle
        run: databricks bundle validate --target "$TARGET"

      # 7) Deploy bundle
      - name: Deploy bundle
        run: databricks bundle deploy --target "$TARGET"

      # 8) Optionally run the job
      - name: Run job after deploy
        if: ${{ github.event.inputs.run_job == 'true' }}
        run: databricks bundle run etl_demo_job --target "$TARGET" --wait
